{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Basic: Carpole\n",
    "\n",
    "The state and the observation are four element vectors:\n",
    "\n",
    "$$\n",
    "o=s=\\left(\\begin{array}{c}\n",
    "x \\\\\n",
    "\\dot{x} \\\\\n",
    "\\theta \\\\\n",
    "\\dot{\\theta}\n",
    "\\end{array}\\right) \\text {, }\n",
    "$$\n",
    "\n",
    "where $x$ is the position of the cart, $\\dot{x}$ is its velocity, $\\theta$ is the angle of the pole w.r.t. the vertical axis, and $\\dot{\\theta}$ is the angular velocity of the pole.\n",
    "\n",
    "In the standard formulation, a reward of 1 is given for every timestep the pole remains balanced. Upon failing (the pole falls) or completing the task, an episode is finished.\n",
    "\n",
    "![carpole](imgs/carpole.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7b321b061f129af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "work_dir = Path().cwd()/'results'\n",
    "import os\n",
    "\n",
    "import train as t \n",
    "import utils as u \n",
    "\n",
    "import numpy as np \n",
    "import torch \n",
    "from IPython.display import Video, display, HTML "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "t.train(cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', \n",
    "        cfg_args=dict(seed=1, max_episode_steps=100))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3286c7202b89218",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "u.plot_reward(Path().cwd()/'results'/'logging'/'CartPole-v1_1.csv', 'CartPole')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2925750bfdf95943",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "t.test(episodes=10, \n",
    "       cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', \n",
    "       cfg_args=dict(testing=True, seed=None, max_episode_steps=1000, use_wandb=False))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fe36a1c7180565",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train Result\n",
    "video_dir = work_dir/'video'/'CartPole-v1'/'train'\n",
    "\n",
    "# List all MP4 files in the directory\n",
    "mp4_files = [file for file in os.listdir(video_dir) if file.endswith(\".mp4\")]\n",
    "frame_colors = ['#FF5733', '#33FF57', '#5733FF', '#FFFF33', '#33FFFF', '#FF33FF']\n",
    "# Display each MP4 file\n",
    "for i, mp4_file in enumerate(mp4_files):\n",
    "    video_path = os.path.join(video_dir, mp4_file)\n",
    "    video = Video(video_path, embed=True, html_attributes=\"loop autoplay\", width=200, height=100)\n",
    "    frame_color = frame_colors[i % len(frame_colors)]\n",
    "    video_frame = HTML(f'<div style=\"width: 200px; height: 100px;; border: 1px solid #FF5733;\">{video._repr_html_()}</div>')\n",
    "    # print(\"test/\",mp4_file)\n",
    "    # display(video_frame)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66564091e3497ac6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Test Result\n",
    "\n",
    "video_dir = work_dir/'video'/'CartPole-v1'/'test'\n",
    "\n",
    "# List all MP4 files in the directory\n",
    "mp4_files = [file for file in os.listdir(video_dir) if file.endswith(\".mp4\")]\n",
    "frame_colors = ['#FF5733', '#33FF57', '#5733FF', '#FFFF33', '#33FFFF', '#FF33FF']\n",
    "# Display each MP4 file\n",
    "for i, mp4_file in enumerate(mp4_files):\n",
    "    video_path = os.path.join(video_dir, mp4_file)\n",
    "    video = Video(video_path, embed=True, html_attributes=\"loop autoplay\", width=200, height=100)\n",
    "    frame_color = frame_colors[i % len(frame_colors)]\n",
    "    video_frame = HTML(f'<div style=\"width: 200px; height: 100px;; border: 1px solid #5733FF;\">{video._repr_html_()}</div>')\n",
    "    # print(\"test/\",mp4_file)\n",
    "    # display(video_frame)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62067d310afb4c79",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extension: Reacher\n",
    "\n",
    "The Cartesian ($x$, $y$) position of the end-effector of the manipulator can be determined following the equation:\n",
    "\n",
    "$$\n",
    " x = L_1 \\sin(\\theta_0)+L_2 \\sin(\\theta_0+\\theta_1)\\\\\n",
    " y = -L_1 \\cos(\\theta_0)-L_2 \\cos(\\theta_0+\\theta_1)\n",
    "$$\n",
    "\n",
    "where $L1 = 1$, $L2 = 1$ are the lengths, and $\\theta_0$, $\\theta_1$ the joint angles of the first and second links respectively. The state (and observation) in this environment is the two element vector:\n",
    "\n",
    "$$\n",
    "o=s=\\left(\\begin{array}{c}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\end{array}\\right) \\text {, }\n",
    "$$\n",
    "\n",
    "The action space now consists of 5 \"options\"; 4 correspond rotating the first/second joint left/right, and the final one performs no motion at all (the configuration doesnâ€™t change). The episode terminates when the agent reaches the target position, marked in red.\n",
    "\n",
    "![reacher](./imgs/reacher.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e417b9f95cde9f35"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from reacher import ReacherEnv\n",
    "from typing import Optional\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "class SpinningReacherEnv(ReacherEnv):\n",
    "    def __init__(self, render_mode: Optional[str] = None, max_episode_steps=200):\n",
    "        super().__init__(render_mode=render_mode, max_episode_steps=max_episode_steps)\n",
    "        \n",
    "    def get_reward(self, prev_state, action, next_state):\n",
    "        reward_theta0 = 1 if next_state[0] > prev_state[0] else -1\n",
    "        reward_theta1 = 1 if next_state[1] > prev_state[1] else -1\n",
    "        total_reward = (reward_theta0 + reward_theta1) / 2.0\n",
    "        return total_reward\n",
    "        \n",
    "    \n",
    "register(\"SpinningReacher-v0\",\n",
    "        entry_point=\"%s:SpinningReacherEnv\"%__name__,\n",
    "        max_episode_steps=200)\n",
    "\n",
    "class TargetReacherEnv(ReacherEnv):\n",
    "    def __init__(self, render_mode: Optional[str] = None, max_episode_steps=200):\n",
    "        super().__init__(render_mode=render_mode, max_episode_steps=max_episode_steps)\n",
    "        \n",
    "    def get_reward(self, prev_state, action, next_state):\n",
    "        target_position = np.array([1.0, 1.0])\n",
    "        current_position = self.get_cartesian_pos(next_state)\n",
    "        \n",
    "        distance = -np.linalg.norm(target_position - current_position)\n",
    "        \n",
    "        if np.isclose(distance, 0, atol=0.05):\n",
    "            return 10\n",
    "        \n",
    "        return distance\n",
    "        \n",
    "register(\"TargetReacher-v0\",\n",
    "        entry_point=\"%s:TargetReacherEnv\"%__name__,\n",
    "        max_episode_steps=200)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2813fbe6d0f421a2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "t.train(cfg_path=Path().cwd()/'cfg'/'reacher_v1.yaml', \n",
    "      cfg_args=dict(env_name='SpinningReacher-v0', train_episodes=200, seed=1))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "602b1262094a2c1b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "t.test(episodes=10, cfg_path=Path().cwd()/'cfg'/'reacher_v1.yaml', \n",
    "       cfg_args=dict(env_name='SpinningReacher-v0', testing=True,))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd33023931cfbc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Video(work_dir/'video'/'SpinningReacher-v0'/'test'/f'ex1-episode-0.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6e6dc571fe3b6ea",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "t.train(cfg_path=Path().cwd()/'cfg'/'reacher_v1.yaml', \n",
    "      cfg_args=dict(env_name='TargetReacher-v0', train_episodes=200, seed=1)) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a3d7fb87a662e74",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "t.test(episodes=10, cfg_path=Path().cwd()/'cfg'/'reacher_v1.yaml', \n",
    "       cfg_args=dict(env_name='TargetReacher-v0', seed=None, testing=True,))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2425aa27aa845505",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Video(work_dir/'video'/'TargetReacher-v0'/'test'/f'ex1-episode-0.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e2cdacebd2b804a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing Behavior "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47b58ab0755eb63a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "from agent import Agent, Policy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8ab0c1615191602",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env_name = \"TargetReacher-v0\" \n",
    "resolution = 101  # Resolution of the policy/reward image\n",
    "\n",
    "# Load policy from default path to plot\n",
    "policy_dir = Path().cwd()/'results'/'model'/f'{env_name}_params.pt'\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# Create a gym environment\n",
    "env = gym.make(env_name)\n",
    "\n",
    "action_space_dim = u.get_space_dim(env.action_space)\n",
    "observation_space_dim = u.get_space_dim(env.observation_space)\n",
    "policy = Policy(observation_space_dim, action_space_dim)\n",
    "\n",
    "if policy_dir:\n",
    "    policy.load_state_dict(torch.load(policy_dir))\n",
    "    print(\"Loading policy from\", policy_dir)\n",
    "else:\n",
    "    print(\"Plotting a random policy\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f666bcb6e6e42336",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create a grid and initialize arrays to store rewards and actions\n",
    "npoints = resolution\n",
    "state_range = np.linspace(-np.pi, np.pi, npoints)\n",
    "rewards = np.zeros((npoints, npoints))\n",
    "actions = np.zeros((npoints, npoints), dtype=np.int32)\n",
    "\n",
    "biggest_rewards = -np.inf\n",
    "best_state = np.array([-np.inf, -np.inf])\n",
    "lowest_rewards = np.inf\n",
    "worst_state = np.array([np.inf, np.inf])\n",
    "\n",
    "# Loop through state[0] and state[1]\n",
    "for i,th1 in enumerate(state_range):\n",
    "    for j,th2 in enumerate(state_range):\n",
    "        # Create the state vector from th1, th2\n",
    "        state = np.array([th1, th2])\n",
    "\n",
    "        # Query the policy and find the most probable action\n",
    "        with torch.no_grad():\n",
    "            action_dist, _ = policy(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        action_probs = action_dist.probs.numpy()\n",
    "        actions[i, j] = np.argmax(action_probs)\n",
    "        pos = env.get_cartesian_pos(state)\n",
    "        rewards[i, j] = -np.sqrt(np.sum((pos - env.goal)**2))\n",
    "        \n",
    "        if rewards[i, j] > biggest_rewards:\n",
    "            biggest_rewards = rewards[i, j]\n",
    "            best_state = state\n",
    "        if rewards[i, j] < lowest_rewards:\n",
    "            lowest_rewards = rewards[i, j]\n",
    "            worst_state = state"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa56ff92b7b863d8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create the reward plot\n",
    "num_ticks = 10\n",
    "tick_skip = max(1, npoints // num_ticks)\n",
    "tick_shift = 2*np.pi/npoints/2\n",
    "tick_points = np.arange(npoints)[::tick_skip] + tick_shift\n",
    "tick_labels = state_range.round(2)[::tick_skip]\n",
    "\n",
    "sns.heatmap(rewards)\n",
    "plt.xticks(tick_points, tick_labels, rotation=45)\n",
    "plt.yticks(tick_points, tick_labels, rotation=45)\n",
    "plt.xlabel(\"J2\")\n",
    "plt.ylabel(\"J1\")\n",
    "plt.title(\"Reward\")\n",
    "plt.suptitle(\"Rewards in %s\" % env_name)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f95b8931bf15115",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create the policy plot\n",
    "cmap = sns.color_palette(\"deep\", action_space_dim)\n",
    "sns.heatmap(actions, cmap=cmap, vmin=0, vmax=action_space_dim-1)\n",
    "plt.xticks(tick_points, tick_labels, rotation=45)\n",
    "plt.yticks(tick_points, tick_labels, rotation=45)\n",
    "colorbar = plt.gca().collections[0].colorbar\n",
    "ticks = np.array(range(action_space_dim))*((action_space_dim-1)/action_space_dim)+0.5\n",
    "colorbar.set_ticks(ticks)\n",
    "if env.spec.id == \"Reacher-v1\":\n",
    "    # In Reacher, we can replace 0..4 with more readable labels\n",
    "    labels = [\"J1+\", \"J1-\", \"J2+\", \"J2-\", \"Stop\"]\n",
    "else:\n",
    "    labels = list(map(str, range(action_space_dim)))\n",
    "colorbar.set_ticklabels(labels)\n",
    "plt.xlabel(\"J2\")\n",
    "plt.ylabel(\"J1\")\n",
    "plt.title(\"Best action\")\n",
    "plt.suptitle(\"Best action in %s\" % env_name)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45df716bb7c6b73",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1e2625cdf1f9daf1",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
